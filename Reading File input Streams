'Shell: 1'
import org.apache.spark.streaming._
val ssc = new StreamingContext(sc, Seconds(2))
val lines = ssc.textFileStream("/tmp/data")
val word = lines.flatMap(_.split(","))
val wordCounts = word.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()

'Shell: 2'
tmp -> cd data/
data -> for i in (1..10)
> do
> cp ../words.txt $Random.txt
> sleep 4
> done
